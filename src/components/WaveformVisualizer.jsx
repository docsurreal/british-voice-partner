import React, { useRef, useEffect, useState } from 'react'\nimport { motion } from 'framer-motion'\n\nexport default function WaveformVisualizer({ isRecording, audioBlob }) {\n  const canvasRef = useRef(null)\n  const animationIdRef = useRef(null)\n  const audioContextRef = useRef(null)\n  const analyserRef = useRef(null)\n  const dataArrayRef = useRef(null)\n  const sourceRef = useRef(null)\n  const [audioUrl, setAudioUrl] = useState(null)\n  const [isPlaying, setIsPlaying] = useState(false)\n  const audioElementRef = useRef(null)\n  \n  useEffect(() => {\n    const canvas = canvasRef.current\n    if (!canvas) return\n    \n    const ctx = canvas.getContext('2d')\n    const width = canvas.width = canvas.offsetWidth * 2\n    const height = canvas.height = canvas.offsetHeight * 2\n    ctx.scale(2, 2)\n    \n    if (isRecording) {\n      startRecordingVisualization()\n    } else if (audioBlob) {\n      const url = URL.createObjectURL(audioBlob)\n      setAudioUrl(url)\n      drawStaticWaveform()\n    } else {\n      drawEmptyState()\n    }\n    \n    return () => {\n      if (animationIdRef.current) {\n        cancelAnimationFrame(animationIdRef.current)\n      }\n      if (audioContextRef.current) {\n        audioContextRef.current.close()\n      }\n      if (audioUrl) {\n        URL.revokeObjectURL(audioUrl)\n      }\n    }\n  }, [isRecording, audioBlob])\n  \n  const startRecordingVisualization = async () => {\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)()\n      analyserRef.current = audioContextRef.current.createAnalyser()\n      sourceRef.current = audioContextRef.current.createMediaStreamSource(stream)\n      \n      analyserRef.current.fftSize = 2048\n      const bufferLength = analyserRef.current.frequencyBinCount\n      dataArrayRef.current = new Uint8Array(bufferLength)\n      \n      sourceRef.current.connect(analyserRef.current)\n      \n      drawRecordingWaveform()\n    } catch (error) {\n      console.error('Error setting up audio visualization:', error)\n      drawEmptyState()\n    }\n  }\n  \n  const drawRecordingWaveform = () => {\n    const canvas = canvasRef.current\n    if (!canvas || !analyserRef.current) return\n    \n    const ctx = canvas.getContext('2d')\n    const width = canvas.offsetWidth\n    const height = canvas.offsetHeight\n    \n    analyserRef.current.getByteTimeDomainData(dataArrayRef.current)\n    \n    ctx.fillStyle = '#0F1419'\n    ctx.fillRect(0, 0, width, height)\n    \n    ctx.lineWidth = 3\n    ctx.strokeStyle = '#FF4B4B'\n    ctx.beginPath()\n    \n    const sliceWidth = width / dataArrayRef.current.length\n    let x = 0\n    \n    for (let i = 0; i < dataArrayRef.current.length; i++) {\n      const v = dataArrayRef.current[i] / 128.0\n      const y = v * height / 2\n      \n      if (i === 0) {\n        ctx.moveTo(x, y)\n      } else {\n        ctx.lineTo(x, y)\n      }\n      \n      x += sliceWidth\n    }\n    \n    ctx.stroke()\n    \n    // Draw frequency bars\n    const freqData = new Uint8Array(analyserRef.current.frequencyBinCount)\n    analyserRef.current.getByteFrequencyData(freqData)\n    \n    const barWidth = width / freqData.length * 2.5\n    let barX = 0\n    \n    ctx.fillStyle = '#FF4B4B'\n    for (let i = 0; i < freqData.length / 4; i++) {\n      const barHeight = (freqData[i] / 255) * height * 0.3\n      \n      ctx.fillRect(barX, height - barHeight, barWidth, barHeight)\n      barX += barWidth + 1\n    }\n    \n    animationIdRef.current = requestAnimationFrame(drawRecordingWaveform)\n  }\n  \n  const drawStaticWaveform = () => {\n    const canvas = canvasRef.current\n    if (!canvas || !audioBlob) return\n    \n    const ctx = canvas.getContext('2d')\n    const width = canvas.offsetWidth\n    const height = canvas.offsetHeight\n    \n    ctx.fillStyle = '#0F1419'\n    ctx.fillRect(0, 0, width, height)\n    \n    // Draw a simulated waveform based on audio blob size and duration\n    ctx.lineWidth = 3\n    ctx.strokeStyle = '#58CC02'\n    ctx.beginPath()\n    \n    const points = 200\n    const sliceWidth = width / points\n    \n    for (let i = 0; i < points; i++) {\n      // Generate pseudo-random waveform based on position\n      const amplitude = Math.sin(i * 0.1) * Math.cos(i * 0.05) * 0.7 + \n                       Math.random() * 0.3\n      const y = height / 2 + amplitude * height / 3\n      const x = i * sliceWidth\n      \n      if (i === 0) {\n        ctx.moveTo(x, y)\n      } else {\n        ctx.lineTo(x, y)\n      }\n    }\n    \n    ctx.stroke()\n    \n    // Add play button overlay\n    const centerX = width / 2\n    const centerY = height / 2\n    const radius = 30\n    \n    ctx.fillStyle = 'rgba(88, 204, 2, 0.8)'\n    ctx.beginPath()\n    ctx.arc(centerX, centerY, radius, 0, 2 * Math.PI)\n    ctx.fill()\n    \n    // Draw play triangle\n    ctx.fillStyle = '#FFFFFF'\n    ctx.beginPath()\n    ctx.moveTo(centerX - 8, centerY - 12)\n    ctx.lineTo(centerX - 8, centerY + 12)\n    ctx.lineTo(centerX + 12, centerY)\n    ctx.closePath()\n    ctx.fill()\n  }\n  \n  const drawEmptyState = () => {\n    const canvas = canvasRef.current\n    if (!canvas) return\n    \n    const ctx = canvas.getContext('2d')\n    const width = canvas.offsetWidth\n    const height = canvas.offsetHeight\n    \n    ctx.fillStyle = '#0F1419'\n    ctx.fillRect(0, 0, width, height)\n    \n    // Draw placeholder waveform\n    ctx.strokeStyle = 'rgba(139, 155, 168, 0.3)'\n    ctx.lineWidth = 2\n    ctx.setLineDash([5, 5])\n    ctx.beginPath()\n    ctx.moveTo(20, height / 2)\n    ctx.lineTo(width - 20, height / 2)\n    ctx.stroke()\n    ctx.setLineDash([])\n    \n    // Draw microphone icon\n    const centerX = width / 2\n    const centerY = height / 2\n    \n    ctx.fillStyle = '#8B9BA8'\n    ctx.fillRect(centerX - 8, centerY - 20, 16, 24)\n    ctx.beginPath()\n    ctx.arc(centerX, centerY + 8, 12, 0, Math.PI)\n    ctx.fill()\n    ctx.fillRect(centerX - 2, centerY + 20, 4, 12)\n    ctx.fillRect(centerX - 12, centerY + 32, 24, 4)\n  }\n  \n  const playRecording = () => {\n    if (!audioUrl || !audioElementRef.current) return\n    \n    if (isPlaying) {\n      audioElementRef.current.pause()\n      setIsPlaying(false)\n    } else {\n      audioElementRef.current.play()\n      setIsPlaying(true)\n    }\n  }\n  \n  return (\n    <div className=\"waveform-container\">\n      <motion.canvas\n        ref={canvasRef}\n        style={{\n          width: '100%',\n          height: '120px',\n          borderRadius: '8px',\n          cursor: audioUrl ? 'pointer' : 'default'\n        }}\n        onClick={audioUrl ? playRecording : undefined}\n        initial={{ opacity: 0 }}\n        animate={{ opacity: 1 }}\n        transition={{ duration: 0.5 }}\n      />\n      \n      {audioUrl && (\n        <>\n          <audio\n            ref={audioElementRef}\n            src={audioUrl}\n            onEnded={() => setIsPlaying(false)}\n            style={{ display: 'none' }}\n          />\n          <div style={{\n            textAlign: 'center',\n            marginTop: '12px',\n            color: '#B7C4CF',\n            fontSize: '14px'\n          }}>\n            {isPlaying ? 'üîä Playing recording...' : '‚ñ∂Ô∏è Click to play recording'}\n          </div>\n        </>\n      )}\n      \n      {isRecording && (\n        <motion.div\n          style={{\n            position: 'absolute',\n            top: '12px',\n            right: '12px',\n            background: '#FF4B4B',\n            color: 'white',\n            padding: '6px 12px',\n            borderRadius: '20px',\n            fontSize: '12px',\n            fontWeight: '600',\n            display: 'flex',\n            alignItems: 'center',\n            gap: '6px'\n          }}\n          animate={{ opacity: [1, 0.5, 1] }}\n          transition={{ repeat: Infinity, duration: 1.5 }}\n        >\n          <div style={{\n            width: '8px',\n            height: '8px',\n            borderRadius: '50%',\n            background: 'white'\n          }} />\n          REC\n        </motion.div>\n      )}\n    </div>\n  )\n}